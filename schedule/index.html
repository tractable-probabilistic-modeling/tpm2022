<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link href="/tpm2022/css/franklin.css" rel=stylesheet > <link href="/tpm2022/css/vela.css" rel=stylesheet > <link href="/tpm2022/css/style.css" rel=stylesheet > <script src="/tpm2022/libs/vela/jquery.min.js"></script> <meta name=format-detection  content="telephone=no" /> <link rel=stylesheet  href="/tpm2022/libs/bootstrap/bootstrap.min.css"> <link href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel=stylesheet > <title>TPM 2022</title> <div class="main-nav slideout-menu slideout-menu-left" id=menu > <div class=flex-container > <span class=sidebar-brand > <h3 style='font-size: 25px'>TPM 2022</h3> </span> </div> <nav class=sidebar-nav > <ul class=metismenu  id=metismenu  > <li><a href="/tpm2022/index.html">Home</a> <li><a href="/tpm2022/cfp/">Call for papers</a> <li><a href="/tpm2022/index.html">Accepted papers</a> <li><a href="/tpm2022/schedule/">Schedule</a> </ul> </nav> </div> <main id=panel  class="slidout-panel slideout-panel-left"> <div class="toggle-button hamburger hamburger--spin"> <div class=hamburger-box > <div class=hamburger-inner ></div> </div> </div> <h1 class="page title">Schedule</h1> <h2 class="page title"><small></small></h2> <h2><center></center></h2> <hr> <div class=franklin-content ><p>The workshop will be held <strong>in-person</strong> on August 5th, 2022 in Eindhoven, Netherlands.</p> <h1 id=tentative_workshop_schedule ><a href="#tentative_workshop_schedule" class=header-anchor >Tentative Workshop Schedule</a></h1> <table><tr><th align=right >Time<th align=right >Description<tr><td align=right >9:00 – 9:15 am<td align=right >Welcome &amp; Best Paper Awards<tr><td align=right >9:15 – 10:00 am<td align=right >Spotlight Presentations<tr><td align=right >10:00 – 11:00 am<td align=right >Poster session I &amp; Coffee Break<tr><td align=right >11:00 – 11:40 am<td align=right >Elizaveta Semenova<tr><td align=right >11:40 – 12:20 pm<td align=right >Rianne van den Berg<tr><td align=right >12:20 – 1:50 pm<td align=right >Lunch Break<tr><td align=right >1:50 – 2:30 pm<td align=right >Vikash Mansinghka<tr><td align=right >2:30 – 3:10 pm<td align=right >Adji Bousso Dieng<tr><td align=right >3:10 – 4:20 pm<td align=right >Poster session II &amp; Coffee Break<tr><td align=right >4:20 – 5:00 pm<td align=right >Vibhav Gogate<tr><td align=right >5:00 – 5:40 pm<td align=right >Baharan Mirzasoleiman<tr><td align=right >5:40 – 6:00 pm<td align=right >Panel discussion &amp; closing remarks</table> <h1 id=invited_talks ><a href="#invited_talks" class=header-anchor >Invited Talks</a></h1> <ul> <li> <b><a href="https://www.microsoft.com/en-us/research/people/rvandenberg/">Rianne van den Berg</a></b> (Microsoft Research) <br/> <b>Title: </b>Generative models for discrete random variables and lossless source compression <br/> <b>Abstract: </b>In this talk I will discuss how different classes of generative models can be adapted to handle discrete random variables, and how this can be used to connect generative models to downstream tasks such as lossless compression. I will start by discussing normalizing flow models, and the challenges that arise when converting these models that are typically designed for real-valued random variables to discrete random variables. Next, I will demonstrate how denoising diffusion models with discrete state spaces have a rich design space in terms of the noising process, and how this influences the performance of the learned denoising model. Finally, I will show how denoising diffusion models can be connected to autoregressive models, and introduce an autoregressive model with a random generation order. <br/> <b>Bio: </b>TBA <br/><br/> <li> <b><a href="https://engineering.princeton.edu/faculty/adji-bousso-dieng">Adji Bousso Dieng</a></b> (Princeton, USA) <br/> <b>Title: </b>TBA <br/> <b>Abstract: </b>TBA <br/> <b>Bio: </b>TBA <br/><br/> <li> <b><a href="https://cs.utdallas.edu/people/faculty/gogate-vibhav/">Vibhav Gogate</a></b> (UT Dallas, USA) <br/> <b>Title: </b>TBA <br/> <b>Abstract: </b>TBA <br/> <b>Bio: </b>TBA <br/><br/> <li> <b><a href="http://probcomp.csail.mit.edu/principal-investigator/">Vikash K. Mansinghka</a></b> (MIT, USA) <br/> <b>Title: </b>TBA <br/> <b>Abstract: </b>TBA <br/> <b>Bio: </b>TBA <br/><br/> <li> <b><a href="https://web.cs.ucla.edu/~baharan/">Baharan Mirzasoleiman</a></b> (UCLA, USA) <br/> <b>Title: </b>Efficient and Robust Learning from Massive Datasets <br/> <b>Abstract: </b>Large datasets have been crucial to the success of modern machine learning models. However, training on massive data has two major limitations. First, it is contingent on exceptionally large and expensive computational resources, and incurs a substantial cost due to the significant energy consumption. Second, in many real-world applications such as medical diagnosis, self-driving cars, and fraud detection, big data contains highly imbalanced classes and noisy labels. In such cases, training on the entire data does not result in a high-quality model. In this talk, I will argue that we can address the above limitations by developing techniques that can identify and extract the most informative subsets for learning from massive datasets. Training on such subsets not only reduces the substantial costs of learning from big data, but also improves their accuracy and robustness against noisy labels. I will discuss how we can develop effective and theoretically rigorous techniques that provide strong guarantees for the learned models’ quality and robustness against noisy labels. <br/> <b>Bio: </b>TBA <br/><br/> <li> <b><a href="https://scholar.google.com/citations?user=jqGIgFEAAAAJ">Elizaveta Semenova</a></b> (University of Oxford, UK) <br/> <b>Title: </b>Encoding spatial priors with VAEs for geospatial modelling <br/> <b>Abstract: </b>Gaussian processes (GPs), implemented through multivariate Gaussian distributions for a finite collection of data, are the most popular approach in spatial statistical modelling. In this context they are used to encode correlation structures over space and can generalise well in interpolation tasks. Despite their flexibility, off-the-shelf GPs present serious computational challenges which limit their scalability and practical usefulness in applied settings. I will present a deep generative modelling approach to tackle this challenge: for a particular spatial setting, a class of GP priors is approximated through prior sampling and subsequent training of a variational autoencoder (VAE). Given a trained VAE, the resultant decoder allows spatial inference to become incredibly efficient due to the low dimensional, independently distributed latent Gaussian space representation of the VAE. Once trained, inference using the VAE decoder replaces the GP within a Bayesian sampling framework. This approach provides tractable and easy-to-implement means of approximately encoding spatial priors and facilitates efficient statistical inference. <br/> <b>Bio: </b>Liza is currently a postdoctoral research associate at the University of Oxford where she works on scalable methods and flexible models for spatiotemporal data. More broadly, interests lie at the intersection of Bayesian statistics and such applied fields as epidemiology, public policy and drug discovery. <br/><br/> </ul> <div class=page-foot > <a href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a> Last modified: June 27, 2022. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a>. </div> </div> </main> <script src="/tpm2022/libs/vela/metisMenu.min.js"></script> <script src="/tpm2022/libs/vela/slideout.min.js"></script>